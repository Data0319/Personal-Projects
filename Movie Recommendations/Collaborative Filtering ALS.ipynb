{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, explode, desc\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "\n",
    "# data science imports\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark config\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"movie recommendation\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"96g\") \\\n",
    "    .config(\"spark.driver.memory\", \"96g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.master\", \"local[12]\") \\\n",
    "    .getOrCreate()\n",
    "# get spark context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.load(os.path.join('/Users/snehal/Documents/Learnings/Python/Data/ml-latest-small/movies.csv'), format='csv', header=True, inferSchema=True)\n",
    "ratings = spark.read.load(os.path.join('/Users/snehal/Documents/Learnings/Python/Data/ml-latest-small/ratings.csv'), format='csv', header=True, inferSchema=True)\n",
    "links = spark.read.load(os.path.join('/Users/snehal/Documents/Learnings/Python/Data/ml-latest-small/links.csv'), format='csv', header=True, inferSchema=True)\n",
    "tags = spark.read.load(os.path.join('/Users/snehal/Documents/Learnings/Python/Data/ml-latest-small/tags.csv'), format='csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "questions:\n",
    "\n",
    "What are the ratings?\n",
    "What is minimum number of ratings per user and minimum number of ratings per movie?\n",
    "How many movies are rated by only one user?\n",
    "What is the total number of users in the data sets?\n",
    "What is the total number of movies in the data sets?\n",
    "How many movies are rated by users? List movies not rated yet?\n",
    "List all movie genres\n",
    "Find out the number of movies for each category\n",
    "Calculate the total rating count for every movie\n",
    "Get a count plot for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(ratings.select('rating').distinct().rdd.map(lambda r: r[0]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the users that rated movies and the movies that were rated:\n",
      "Minimum number of ratings per user is 20\n",
      "Minimum number of ratings per movie is 1\n"
     ]
    }
   ],
   "source": [
    "tmp1 = ratings.groupBy(\"userID\").count().toPandas()['count'].min()\n",
    "tmp2 = ratings.groupBy(\"movieId\").count().toPandas()['count'].min()\n",
    "print('For the users that rated movies and the movies that were rated:')\n",
    "print('Minimum number of ratings per user is {}'.format(tmp1))\n",
    "print('Minimum number of ratings per movie is {}'.format(tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3446 out of 9724 movies are rated by only one user\n"
     ]
    }
   ],
   "source": [
    "tmp1 = sum(ratings.groupBy(\"movieId\").count().toPandas()['count'] == 1)\n",
    "tmp2 = ratings.select('movieId').distinct().count()\n",
    "print('{} out of {} movies are rated by only one user'.format(tmp1, tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 610 distinct users in the data sets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tmp = ratings.select('userID').distinct().count()\n",
    "print('We have a total of {} distinct users in the data sets'.format(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 9742 distinct movies in the data sets\n"
     ]
    }
   ],
   "source": [
    "tmp = movies.select('movieID').distinct().count()\n",
    "print('We have a total of {} distinct movies in the data sets'.format(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 9724 distinct movies that are rated by users in ratings table\n",
      "We have 18 movies that are not rated yet\n"
     ]
    }
   ],
   "source": [
    "tmp1 = movies.select('movieID').distinct().count()\n",
    "tmp2 = ratings.select('movieID').distinct().count()\n",
    "print('We have a total of {} distinct movies that are rated by users in ratings table'.format(tmp2))\n",
    "print('We have {} movies that are not rated yet'.format(tmp1-tmp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List movies that are not rated yet: \n",
      "+-------+--------------------+\n",
      "|movieId|               title|\n",
      "+-------+--------------------+\n",
      "|   1076|Innocents, The (1...|\n",
      "|   2939|      Niagara (1953)|\n",
      "|   3338|For All Mankind (...|\n",
      "|   3456|Color of Paradise...|\n",
      "|   4194|I Know Where I'm ...|\n",
      "|   5721|  Chosen, The (1981)|\n",
      "|   6668|Road Home, The (W...|\n",
      "|   6849|      Scrooge (1970)|\n",
      "|   7020|        Proof (1991)|\n",
      "|   7792|Parallax View, Th...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a temp SQL table view for easier query\n",
    "movies.createOrReplaceTempView(\"movies\")\n",
    "ratings.createOrReplaceTempView(\"ratings\")\n",
    "print('List movies that are not rated yet: ')\n",
    "# SQL query (NOTE: WHERE ... NOT IN ... == ... LEFT JOIN ... WHERE ... IS NULL)\n",
    "# Approach 1\n",
    "spark.sql(\n",
    "    \"SELECT movieId, title \"\n",
    "    \"FROM movies \"\n",
    "    \"WHERE movieId NOT IN (SELECT distinct(movieId) FROM ratings)\"\n",
    ").show(10)\n",
    "# Approach 2\n",
    "# spark.sql(\n",
    "#     \"SELECT m.movieId, m.title \"\n",
    "#     \"FROM movies m LEFT JOIN ratings r ON m.movieId=r.movieId \"\n",
    "#     \"WHERE r.movieId IS NULL\"\n",
    "# ).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All distinct genres: \n",
      "+------------------+\n",
      "|            genres|\n",
      "+------------------+\n",
      "|             Crime|\n",
      "|           Romance|\n",
      "|          Thriller|\n",
      "|         Adventure|\n",
      "|             Drama|\n",
      "|               War|\n",
      "|       Documentary|\n",
      "|           Fantasy|\n",
      "|           Mystery|\n",
      "|           Musical|\n",
      "|         Animation|\n",
      "|         Film-Noir|\n",
      "|(no genres listed)|\n",
      "|              IMAX|\n",
      "|            Horror|\n",
      "|           Western|\n",
      "|            Comedy|\n",
      "|          Children|\n",
      "|            Action|\n",
      "|            Sci-Fi|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitter = UserDefinedFunction(lambda x: x.split('|'), ArrayType(StringType()))\n",
    "# query\n",
    "print('All distinct genres: ')\n",
    "movies.select(explode(splitter(\"genres\")).alias(\"genres\")).distinct().show()\n",
    "# All distinct genres: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of movies per genre\n",
      "+------------------+-----+\n",
      "|            genres|count|\n",
      "+------------------+-----+\n",
      "|             Drama| 4361|\n",
      "|            Comedy| 3756|\n",
      "|          Thriller| 1894|\n",
      "|            Action| 1828|\n",
      "|           Romance| 1596|\n",
      "|         Adventure| 1263|\n",
      "|             Crime| 1199|\n",
      "|            Sci-Fi|  980|\n",
      "|            Horror|  978|\n",
      "|           Fantasy|  779|\n",
      "|          Children|  664|\n",
      "|         Animation|  611|\n",
      "|           Mystery|  573|\n",
      "|       Documentary|  440|\n",
      "|               War|  382|\n",
      "|           Musical|  334|\n",
      "|           Western|  167|\n",
      "|              IMAX|  158|\n",
      "|         Film-Noir|   87|\n",
      "|(no genres listed)|   34|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Counts of movies per genre')\n",
    "movies.select('movieID', explode(splitter(\"genres\")).alias(\"genres\")) \\\n",
    "    .groupby('genres') \\\n",
    "    .count() \\\n",
    "    .sort(desc('count')) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 4.0), (1, 3, 4.0), (1, 6, 4.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "movie_rating = sc.textFile('/Users/snehal/Documents/Learnings/Python/Data/ml-latest-small/ratings.csv')\n",
    "# preprocess data -- only need [\"userId\", \"movieId\", \"rating\"]\n",
    "header = movie_rating.take(1)[0]\n",
    "rating_data = movie_rating \\\n",
    "    .filter(lambda line: line!=header) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda tokens: (int(tokens[0]), int(tokens[1]), float(tokens[2]))) \\\n",
    "    .cache()\n",
    "# check three rows\n",
    "rating_data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[122] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, validation, test = rating_data.randomSplit([6, 2, 2], seed=99)\n",
    "# cache data\n",
    "train.cache()\n",
    "validation.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_ALS(train_data, validation_data, num_iters, reg_param, ranks):\n",
    "    \"\"\"\n",
    "    Grid Search Function to select the best model based on RMSE of hold-out data\n",
    "    \"\"\"\n",
    "    # initial\n",
    "    min_error = float('inf')\n",
    "    best_rank = -1\n",
    "    best_regularization = 0\n",
    "    best_model = None\n",
    "    for rank in ranks:\n",
    "        for reg in reg_param:\n",
    "            # train ALS model\n",
    "            model = ALS.train(\n",
    "                ratings=train_data,    # (userID, productID, rating) tuple\n",
    "                iterations=num_iters,\n",
    "                rank=rank,\n",
    "                lambda_=reg,           # regularization param\n",
    "                seed=99)\n",
    "            # make prediction\n",
    "            valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "            predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "            # get the rating result\n",
    "            ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "            # get the RMSE\n",
    "            MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "            error = math.sqrt(MSE)\n",
    "            print('{} latent factors and regularization = {}: validation RMSE is {}'.format(rank, reg, error))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                best_rank = rank\n",
    "                best_regularization = reg\n",
    "                best_model = model\n",
    "    print('\\nThe best model has {} latent factors and regularization = {}'.format(best_rank, best_regularization))\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 latent factors and regularization = 0.001: validation RMSE is 1.3802413929920245\n",
      "8 latent factors and regularization = 0.01: validation RMSE is 1.152578168188089\n",
      "8 latent factors and regularization = 0.05: validation RMSE is 0.979302926362013\n",
      "8 latent factors and regularization = 0.1: validation RMSE is 0.9107684352784347\n",
      "8 latent factors and regularization = 0.2: validation RMSE is 0.8921656987096\n",
      "10 latent factors and regularization = 0.001: validation RMSE is 1.4370917084334793\n",
      "10 latent factors and regularization = 0.01: validation RMSE is 1.1924782886146474\n",
      "10 latent factors and regularization = 0.05: validation RMSE is 0.9882870648737777\n",
      "10 latent factors and regularization = 0.1: validation RMSE is 0.9115834506125544\n",
      "10 latent factors and regularization = 0.2: validation RMSE is 0.8927502946108398\n",
      "12 latent factors and regularization = 0.001: validation RMSE is 1.4893887488719786\n",
      "12 latent factors and regularization = 0.01: validation RMSE is 1.238240763097585\n",
      "12 latent factors and regularization = 0.05: validation RMSE is 1.0024402018493226\n",
      "12 latent factors and regularization = 0.1: validation RMSE is 0.9148714420422935\n",
      "12 latent factors and regularization = 0.2: validation RMSE is 0.8915894865259544\n",
      "14 latent factors and regularization = 0.001: validation RMSE is 1.5299396835579941\n",
      "14 latent factors and regularization = 0.01: validation RMSE is 1.2684385995680674\n",
      "14 latent factors and regularization = 0.05: validation RMSE is 1.001751146607926\n",
      "14 latent factors and regularization = 0.1: validation RMSE is 0.911309430292058\n",
      "14 latent factors and regularization = 0.2: validation RMSE is 0.8933548390736112\n",
      "16 latent factors and regularization = 0.001: validation RMSE is 1.6122245856554802\n",
      "16 latent factors and regularization = 0.01: validation RMSE is 1.3117787284884408\n",
      "16 latent factors and regularization = 0.05: validation RMSE is 1.009913414851606\n",
      "16 latent factors and regularization = 0.1: validation RMSE is 0.9130907774116139\n",
      "16 latent factors and regularization = 0.2: validation RMSE is 0.8935418208791914\n",
      "18 latent factors and regularization = 0.001: validation RMSE is 1.6275771926387341\n",
      "18 latent factors and regularization = 0.01: validation RMSE is 1.3154712781018496\n",
      "18 latent factors and regularization = 0.05: validation RMSE is 1.0057356004906293\n",
      "18 latent factors and regularization = 0.1: validation RMSE is 0.914579068975207\n",
      "18 latent factors and regularization = 0.2: validation RMSE is 0.8941924525166205\n",
      "20 latent factors and regularization = 0.001: validation RMSE is 1.665944065258818\n",
      "20 latent factors and regularization = 0.01: validation RMSE is 1.3291689104211313\n",
      "20 latent factors and regularization = 0.05: validation RMSE is 1.0051002321850793\n",
      "20 latent factors and regularization = 0.1: validation RMSE is 0.9104337441317789\n",
      "20 latent factors and regularization = 0.2: validation RMSE is 0.8931677508967799\n",
      "\n",
      "The best model has 12 latent factors and regularization = 0.2\n",
      "Total Runtime: 231.99 seconds\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "ranks = [8, 10, 12, 14, 16, 18, 20]\n",
    "reg_params = [0.001, 0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "# grid search and select best model\n",
    "start_time = time.time()\n",
    "final_model = train_ALS(train, validation, num_iterations, reg_params, ranks)\n",
    "\n",
    "print ('Total Runtime: {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(arr_iters, train_data, validation_data, reg, rank):\n",
    "    \"\"\"\n",
    "    Plot function to show learning curve of ALS\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for num_iters in arr_iters:\n",
    "        # train ALS model\n",
    "        model = ALS.train(\n",
    "            ratings=train_data,    # (userID, productID, rating) tuple\n",
    "            iterations=num_iters,\n",
    "            rank=rank,\n",
    "            lambda_=reg,           # regularization param\n",
    "            seed=99)\n",
    "        # make prediction\n",
    "        valid_data = validation_data.map(lambda p: (p[0], p[1]))\n",
    "        predictions = model.predictAll(valid_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "        # get the rating result\n",
    "        ratesAndPreds = validation_data.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "        # get the RMSE\n",
    "        MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "        error = math.sqrt(MSE)\n",
    "        # add to errors\n",
    "        errors.append(error)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(arr_iters, errors)\n",
    "    plt.xlabel('number of iterations')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('ALS Learning Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of num_iters\n",
    "iter_array = list(range(1, 11))\n",
    "# create learning curve plot\n",
    "plot_learning_curve(iter_array, train, validation, 0.05, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction using test data\n",
    "test_data = test.map(lambda p: (p[0], p[1]))\n",
    "predictions = final_model.predictAll(test_data).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "# get the rating result\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "# get the RMSE\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "error = math.sqrt(MSE)\n",
    "print('The out-of-sample RMSE of rating predictions is', round(error, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieId(df_movies, fav_movie_list):\n",
    "    \"\"\"\n",
    "    return all movieId(s) of user's favorite movies\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    movieId_list: list of movieId(s)\n",
    "    \"\"\"\n",
    "    movieId_list = []\n",
    "    for movie in fav_movie_list:\n",
    "        movieIds = df_movies \\\n",
    "            .filter(movies.title.like('%{}%'.format(movie))) \\\n",
    "            .select('movieId') \\\n",
    "            .rdd \\\n",
    "            .map(lambda r: r[0]) \\\n",
    "            .collect()\n",
    "        movieId_list.extend(movieIds)\n",
    "    return list(set(movieId_list))\n",
    "\n",
    "\n",
    "def add_new_user_to_data(train_data, movieId_list, spark_context):\n",
    "    \"\"\"\n",
    "    add new rows with new user, user's movie and ratings to\n",
    "    existing train data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    new train data with the new user's rows\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # get max rating\n",
    "    max_rating = train_data.map(lambda r: r[2]).max()\n",
    "    # create new user rdd\n",
    "    user_rows = [(new_id, movieId, max_rating) for movieId in movieId_list]\n",
    "    new_rdd = spark_context.parallelize(user_rows)\n",
    "    # return new train data\n",
    "    return train_data.union(new_rdd)\n",
    "\n",
    "\n",
    "def get_inference_data(train_data, df_movies, movieId_list):\n",
    "    \"\"\"\n",
    "    return a rdd with the userid and all movies (except ones in movieId_list)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "    \n",
    "    movieId_list: list, list of movieId(s)\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    inference data: Spark RDD\n",
    "    \"\"\"\n",
    "    # get new user id\n",
    "    new_id = train_data.map(lambda r: r[0]).max() + 1\n",
    "    # return inference rdd\n",
    "    return df_movies.rdd \\\n",
    "        .map(lambda r: r[0]) \\\n",
    "        .distinct() \\\n",
    "        .filter(lambda x: x not in movieId_list) \\\n",
    "        .map(lambda x: (new_id, x))\n",
    "\n",
    "\n",
    "def make_recommendation(best_model_params, ratings_data, df_movies, \n",
    "                        fav_movie_list, n_recommendations, spark_context):\n",
    "    \"\"\"\n",
    "    return top n movie recommendation based on user's input list of favorite movies\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    best_model_params: dict, {'iterations': iter, 'rank': rank, 'lambda_': reg}\n",
    "\n",
    "    ratings_data: spark RDD, ratings data\n",
    "\n",
    "    df_movies: spark Dataframe, movies data\n",
    "\n",
    "    fav_movie_list: list, user's list of favorite movies\n",
    "\n",
    "    n_recommendations: int, top n recommendations\n",
    "\n",
    "    spark_context: Spark Context object\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    list of top n movie recommendations\n",
    "    \"\"\"\n",
    "    # modify train data by adding new user's rows\n",
    "    movieId_list = get_movieId(df_movies, fav_movie_list)\n",
    "    train_data = add_new_user_to_data(ratings_data, movieId_list, spark_context)\n",
    "    \n",
    "    # train best ALS\n",
    "    model = ALS.train(\n",
    "        ratings=train_data,\n",
    "        iterations=best_model_params.get('iterations', None),\n",
    "        rank=best_model_params.get('rank', None),\n",
    "        lambda_=best_model_params.get('lambda_', None),\n",
    "        seed=99)\n",
    "    \n",
    "    # get inference rdd\n",
    "    inference_rdd = get_inference_data(ratings_data, df_movies, movieId_list)\n",
    "    \n",
    "    # inference\n",
    "    predictions = model.predictAll(inference_rdd).map(lambda r: (r[1], r[2]))\n",
    "    \n",
    "    # get top n movieId\n",
    "    topn_rows = predictions.sortBy(lambda r: r[1], ascending=False).take(n_recommendations)\n",
    "    topn_ids = [r[0] for r in topn_rows]\n",
    "    \n",
    "    # return movie titles\n",
    "    return df_movies.filter(movies.movieId.isin(topn_ids)) \\\n",
    "                    .select('title') \\\n",
    "                    .rdd \\\n",
    "                    .map(lambda r: r[0]) \\\n",
    "                    .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my favorite movies\n",
    "my_favorite_movies = ['Iron Man']\n",
    "\n",
    "# get recommends\n",
    "recommends = make_recommendation(\n",
    "    best_model_params={'iterations': 10, 'rank': 20, 'lambda_': 0.05}, \n",
    "    ratings_data=rating_data, \n",
    "    df_movies=movies, \n",
    "    fav_movie_list=my_favorite_movies, \n",
    "    n_recommendations=10, \n",
    "    spark_context=sc)\n",
    "\n",
    "print('Recommendations for {}:'.format(my_favorite_movies[0]))\n",
    "for i, title in enumerate(recommends):\n",
    "    print('{0}: {1}'.format(i+1, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of movie recommendations look completely different than the list from my previous KNN model recommender. Not only it recommends movies outside of years between 2007 and 2009 periods, but also recommends movies that were less known. So this can offer users some elements of suprise so that users won't get bored by getting the same popular movies all the tim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
